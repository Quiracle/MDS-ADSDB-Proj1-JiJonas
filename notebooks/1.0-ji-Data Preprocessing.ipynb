{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset have different keywords, and different time-stamps for the date when they are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kewords = [\"renda familiar\", \"idealista\", \"extended\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "renda = pd.read_excel(\"../temporal_landing/renda familiar 2020.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file for 2020 saved as ../persistent_landing/2020_idealista.json\n",
      "Merged file for 2021 saved as ../persistent_landing/2021_idealista.json\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the directory where your JSON files are stored\n",
    "temporal_directory = '../temporal_landing/'\n",
    "persistent_directory = '../persistent_landing/'\n",
    "\n",
    "# Dictionary to hold DataFrames for each year\n",
    "dfs_by_year = defaultdict(list)\n",
    "\n",
    "# Loop through all the files in the directory\n",
    "for filename in os.listdir(temporal_directory):\n",
    "    # Check if the filename matches the pattern YYYY_MM_dd_idealista.json\n",
    "    if filename.endswith('_idealista.json'):\n",
    "        try:\n",
    "            # Extract the year, month, and day from the filename\n",
    "            # Assuming the filename format is YYYY_MM_dd_idealista.json\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) < 4:\n",
    "                print(f\"Filename {filename} does not match the expected pattern. Skipping.\")\n",
    "                continue\n",
    "            year, month, day = parts[0], parts[1], parts[2]\n",
    "            \n",
    "            # Validate extracted parts\n",
    "            datetime.strptime(f\"{year}_{month}_{day}\", \"%Y_%m_%d\")  # This will raise ValueError if invalid\n",
    "            \n",
    "            # Create a date string or datetime object as needed\n",
    "            extraction_date = f\"{year}-{month}-{day}\"  # Format: YYYY-MM-DD\n",
    "            # Alternatively, as a datetime object:\n",
    "            # extraction_date = datetime.strptime(f\"{year}_{month}_{day}\", \"%Y_%m_%d\")\n",
    "            \n",
    "            # Read the JSON file\n",
    "            file_path = os.path.join(temporal_directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                # Check if the data is a list of dictionaries, convert it to a DataFrame\n",
    "                if isinstance(data, list):\n",
    "                    df = pd.DataFrame(data)\n",
    "                elif isinstance(data, dict):\n",
    "                    # If it's a dictionary, convert it to a DataFrame (handle accordingly)\n",
    "                    df = pd.DataFrame([data])\n",
    "                else:\n",
    "                    print(f\"Unexpected data format in {filename}. Skipping.\")\n",
    "                    continue\n",
    "            \n",
    "            # Add the extraction date as a new column\n",
    "            df['extraction_timestamp'] = extraction_date\n",
    "            \n",
    "            # Append the DataFrame to the list corresponding to the year\n",
    "            dfs_by_year[year].append(df)\n",
    "        \n",
    "        except ValueError as ve:\n",
    "            print(f\"Error processing filename {filename}: {ve}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "columns_to_remove = ['thumbnail', 'externalReference', \"numPhotos\", \"showAddress\", \n",
    "                     \"url\", \"distance\", \"hasVideo\", \"detailedType\", \"suggestedTexts\", \n",
    "                     \"hasPlan\", \"has3DTour\", \"has360\", \"hasStaging\", \"parkingSpace\",\n",
    "                     \"topNewDevelopment\", \"newDevelopmentFinished\", ]\n",
    "\n",
    "# Loop through each year, merge the DataFrames, remove duplicates, and save to a new JSON file\n",
    "for year, dfs in dfs_by_year.items():\n",
    "    # Concatenate all DataFrames for the given year\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Drop the unwanted columns\n",
    "    combined_df = combined_df.drop(columns=columns_to_remove)\n",
    "\n",
    "    # Save the combined DataFrame to a new JSON file\n",
    "    output_file = os.path.join(persistent_directory, f'{year}_idealista.json')\n",
    "    combined_df.to_json(output_file, orient='records', indent=4)\n",
    "    \n",
    "    print(f'Merged file for {year} saved as {output_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
